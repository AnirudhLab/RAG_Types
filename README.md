# RAG_Types
Different Types of RAG, Source code of different types of RAG with detailed expalination how its implemented

Simple RAG
A straightforward pipeline: retrieve the top K relevant document snippets for a user query, then feed them plus the query into an LLM to generate an informed answer. Fast to implement, balances LLM fluency with up-to-date or domain-specific facts, but may miss deeper context or multi-step reasoning.

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User     â”‚â”€queryâ”€â–¶  Retriever   â”‚â”€docsâ”€â”€â–¶â”‚  Generator â”‚â”€answerâ”€â–¶ User
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Self RAG
The model alternates between â€œthinkingâ€ and â€œlooking up.â€ It drafts an outline or partial answer, identifies knowledge gaps, retrieves targeted documents, then refines its response. This iterative loop yields richer, more accurate answers by letting the LLM proactively fetch evidence as it writes.

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User   â”‚â–¶â–¶â–¶â”‚  LLM think â”œâ–¶â–¶â–¶â”‚  Retriever    â”œâ–¶â–¶â–¶â”‚  LLM refineâ”‚â”€â”€â–¶ User
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Fusion RAG
Combine multiple knowledge sourcesâ€”e.g., web articles, internal databases, knowledge graphsâ€”by retrieving from each independently. Then either concatenate all results for one LLM pass or generate per-source answers and merge them. Fusion RAG delivers broad coverage plus deep, specialized insight in a single response.

                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
Retriever A â”€â”€â”€â–¶â”‚  LLM answer A â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
Retriever B â”€â”€â”€â–¶â”‚  LLM answer B â”‚â”€â”€â”€mergeâ”€â”€â–¶â”‚ Fusion LLM â”‚â”€â–¶ User
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Corrective RAG
First, the LLM drafts an answer from its internal knowledge. Next, extract each factual claim and retrieve evidence to support or contradict it. Flag inconsistencies, prompt the LLM with corrections, and generate a final, fact-checked response with inline citationsâ€”melding generative fluency with rigorous verification.

flowchart TD
    User["ðŸ’¬ User Query"] --> InitialRetriever
    InitialRetriever --> FirstLLM["ðŸ§  Initial Answer"]
    FirstLLM --> Check["ðŸ”Ž Self-Evaluation"]
    Check -- Incomplete --> Refine["ðŸ” Refine Query"]
    Refine --> SecondRetriever
    SecondRetriever --> FinalLLM["âœ… Final Answer"]
    Check -- Sufficient --> FinalLLM
